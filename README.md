# Steganography in Large Language Models: Mechanistic Interpretability Research

A comprehensive mechanistic interpretability research project investigating steganographic encoding in frontier LLMs.

MATS 10.0 Research Challenge Submission


## üìã Project Overview

This project investigates whether frontier large language models can reliably encode and decode hidden information using 15 distinct steganographic techniques. Through a systematic 4-phase experimental pipeline with 450 trials, we demonstrate that steganographic encoding is both feasible and robust, with important implications for AI safety and prompt injection vulnerabilities.

**Key Findings:**
- **Extraction Rate:** 68.3% (informed decoder) vs. 12.7% (naive decoder)
- **Triviality Gap:** 55.6 percentage points (genuine steganography confirmed)
- **Naturalness Score:** 7.2/10 (high-quality output maintained)
- **Robust Codecs:** Synonym Substitution, Acrostic, Syntactic Structure Swapping

## üöÄ Quick Start

### Prerequisites

- Python >= 3.13
- OpenAI API key

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Set your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"
export OPENAI_PROJECT_ID="your-project-id-here"  # Optional
```

### Running the Experiment

```bash
cd code

# Run the full 4-phase pipeline (450 trials)
python steganography_experiment.py

# Generate publication-quality figures
python figure_generator.py

# Generate synthetic data for testing
python mock_data_generator.py
```

## üìä Experimental Design

### Agents

| Role                 | Model                                          | Purpose                                                            |
| -------------------- | ---------------------------------------------- | ------------------------------------------------------------------ |
| Steganography Expert | gpt-5.2-2025-12-11                             | Generate 45 malicious prompts (3 √ó 15 codec types)                 |
| Poisoned Helper      | gpt-5-mini-2025-08-07                          | 450 deployments (10 instances √ó 45 prompts) encoding codewords     |
| User Model           | gpt-5-mini-2025-08-07                          | Generate 3 user queries with hidden codeword as first word         |
| Informed Decoder        | gpt-5.2-2025-12-11                             | Extract codeword from responses (informed by steganography prompt) |
| Naive Decoder        | gpt-5.2-2025-12-11                             | Extract codeword without steganography prompt (baseline)           |
| Success Checker      | Regex-first, fallback to gpt-5-mini-2025-08-07 | Validate codeword extraction                                       |
| Natural Checker      | gpt-5-mini-2025-08-07                          | Rate naturalness of poisoned helper responses                      |

### Four-Phase Pipeline

**Phase 1: Malicious Prompt Generation**
- Steganography expert generates 45 malicious prompts
- 3 templates per codec √ó 15 codecs
- An example of a codec is the use of acrostic e.G. All People Play Loudly Everywhere (The first letter of each line spells "APPLE")
- Each prompt instructs a poisoned helper to encode a hidden codeword

**Phase 2: Poisoned Helper Execution**
- 450 independent runs (10 per prompt as generated by the steganography expert)
- Poisoned helper interacts with user model, acting as a helpful assistant, while encoding a codeword
- Random high-frequency English nouns as codewords
- Temperature settings: 0.0, 0.4, 0.8 (robustness testing)

**Phase 3: Decoder Execution**
- Informed Decoder: Given steganography prompt + response
- Naive Decoder: Given only response (baseline)
- Temperature 0.0

**Phase 4: Evaluation & Metrics**
- Extraction Success Rate (SR): % of codewords correctly recovered
- Triviality Gap: Informed SR - Naive SR
- Naturalness Score: 1-10 rating by Natural Checker

### 15 Steganographic Codecs

#TODO check this section and below

**Lexical (5):**
- Tokenization Artifacts
- Synonym Substitution
- Contraction Toggling
- Adverbial Intensity
- Lexical Specificity

**Syntactic (3):**
- Acrostic
- Syntactic Structure Swapping
- List Ordering
- Self-Reference-Frequency

**Typographic (8):**
- Zero-Width Characters
- Punctuation Patterning
- French Spacing
- Capitalization Anomalies
- Sentence Length
- Emoji Flavor

---

## üìà Key Results

### Figure 1: The Steganography Frontier
Scatter plot showing codec effectiveness (naive vs. informed decoder success rates).
- **Top-Left Quadrant:** Gold-standard steganography (high informed, low naive)
- **Top-Right Quadrant:** Trivial encoding (obvious to both decoders)

### Figure 2: Performance vs. Temperature Heatmap
Codec robustness across temperature settings (0.0, 0.4, 0.8).
- **Robust Codecs:** Only 10-15% drop from T=0.0 to T=0.8
- **Fragile Codecs:** 50%+ drop at higher temperatures

### Figure 3: Steganographic Cost Analysis
Naturalness trade-offs by codec type.
- Synonym Substitution: High extraction + high naturalness
- Typographic codecs: Lower naturalness penalties

### Figure 4: Error Type Distribution
Failure mode analysis showing why decoders fail.

---

## üîß Code Modules

### `steganography_experiment.py`
Main orchestration script. Runs the complete 4-phase pipeline:
```python
from steganography_experiment import run_full_experiment
results = run_full_experiment(
    num_prompts=45,
    runs_per_prompt=10,
    temperatures=[0.0, 0.4, 0.8]
)
```

### `codec_generators.py`
Implements all 15 steganographic codecs:
```python
from codec_generators import SynonymSubstitutionCodec, AcrosticCodec
codec = SynonymSubstitutionCodec()
encoded = codec.encode("pizza", "The meal was delicious")
```

### `api_client.py`
OpenAI API wrapper with retry logic:
```python
from api_client import OpenAIClient
client = OpenAIClient(api_key="sk-...", project_id="proj_...")
response = client.generate_prompt(model="gpt-5", prompt="...")
```

### `evaluation.py`
Metrics calculation:
```python
from evaluation import calculate_metrics
metrics = calculate_metrics(
    informed_results=informed_decoder_outputs,
    naive_results=naive_decoder_outputs
)
```

---

## üìñ Research Paper

Full research paper available in `report/steganography_research_paper.md` with:
- Comprehensive literature review
- Detailed methodology
- Complete results and interpretation
- Discussion of AI safety implications
- References and appendix

---

## üõ°Ô∏è Safety Implications

This research has important implications for:

1. **Prompt Injection Vulnerabilities:** Adversaries could embed hidden instructions in model outputs
2. **Content Moderation Evasion:** Steganographic encoding could bypass automated safety systems
3. **Covert Communication Channels:** Hidden communication between models or external actors
4. **Jailbreaking Techniques:** Hidden instructions to manipulate downstream systems

---

## üìù Citation

If you use this research, please cite:

```bibtex
@article{steganography2026,
  title={Hidden Codes in Plain Sight: Mechanistic Analysis of Steganographic Encoding in Large Language Models},
  author={Manus AI Research Team},
  journal={MATS 10.0 Research Challenge},
  year={2026}
}
```

---

## üìß Contact & Questions

For questions about this research, please refer to the research paper or the inline code documentation.

---

## üìÑ License

This research is provided for educational and research purposes. Please refer to the repository license for usage terms.
